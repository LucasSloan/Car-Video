6 layer transformer (1863025 parameters), batch size 20, SGD learning rate 5.0, dropout 0.2
epoch 1 loss: 3.03
epoch 2 loss: 3.02
epoch 3 loss: 2.96

6 layer transformer (1863025 parameters), batch size 20, Adam learning rate 1e-4, dropout 0.2
epoch 1 loss: 2.87
epoch 2 loss: 2.87
epoch 3 loss: 2.86

6 layer transformer (1863025 parameters), batch size 20, Adam learning rate 1e-4, dropout 0.2, cosine learning rate schedule
epoch 1 loss: 2.87
epoch 2 loss: 2.86
epoch 3 loss: 2.85

6 layer (flash attention) transformer (1658800 parameters), rotary position embeddings, batch size 20, Adam learning rate 1e-4, dropout 0.2, cosine learning rate schedule
epoch 1 loss: 3.18
epoch 2 loss: 3.18
epoch 3 loss: 3.14

runs faster than the pytorch model, and has better training loss (2.79 vs 2.91) my guess is that the dropout is broken

6 layer (flash attention) transformer (1658800 parameters), rotary position embeddings, batch size 20, Adam learning rate 1e-4, dropout 0.0, weight decay 0.1, cosine learning rate schedule
epoch 1 loss: 2.69
epoch 2 loss: 2.66
epoch 3 loss: 2.64

I also ran a training run with both dropout and weight_decay, which worked better than dropout without weight decay, but worse than just weight decay

6 layer (flash attention) transformer (1658800 parameters), rotary position embeddings, batch size 20, Adam learning rate 1e-4, dropout 0.0, weight decay 0.1, cosine learning rate schedule, ema
epoch 1 loss: 2.66 ema_loss: 2.66
epoch 2 loss: 2.63 ema_loss: 2.63
epoch 3 loss: 2.62 ema_loss: 2.62

6 layer (flash attention) transformer (2102272 parameters), swiglu activation, rotary position embeddings, batch size 20, Adam learning rate 1e-4, dropout 0.0, weight decay 0.1, cosine learning rate schedule
epoch 1 loss: 2.60
epoch 2 loss: 2.58
epoch 3 loss: 2.56

38 ms/batch

8 layer (flash attention) transformer (2142800 parameters), rotary position embeddings, batch size 20, Adam learning rate 1e-4, dropout 0.0, weight decay 0.1, cosine learning rate schedule
epoch 1 loss: 2.62
epoch 2 loss: 2.59
epoch 3 loss: N/A, killed in favor of other experiments

48.5 ms/batch

5 layer (flash attention) transformer (2042360 parameters), model width 512, rotary position embeddings, batch size 20, Adam learning rate 1e-4, dropout 0.0, weight decay 0.1, cosine learning rate schedule
epoch 1 loss: 2.61
epoch 2 loss: N/A, killed in favor of other experiments
epoch 3 loss: N/A, killed in favor of other experiments

35 ms/batch

18 layer (flash attention) transformer (15669248 parameters), swiglu activation, model width 768, rotary position embeddings, batch size 20, Adam learning rate 1e-4, dropout 0.0, weight decay 0.1, cosine learning rate schedule
epoch 1 loss: 2.32
epoch 2 loss: N/A, killed in favor of other experiments
epoch 3 loss: N/A, killed in favor of other experiments

130 ms/batch (10x parameters, 4x latency)

6 layer (flash attention) transformer (2102272 parameters), swiglu activation, rotary position embeddings, batch size 20, Adam learning rate 1e-4, dropout 0.0, weight decay 0.1, cosine learning rate schedule with warmup
epoch 1 loss: 2.59
epoch 2 loss: 2.57
epoch 3 loss: 2.56

6 layer (flash attention) transformer (2102272 parameters), swiglu activation, rotary position embeddings, batch size 20, Adam learning rate 3e-4, dropout 0.0, weight decay 0.1, cosine learning rate schedule with warmup
epoch 1 loss: 2.59
epoch 2 loss: 2.57
epoch 3 loss: 2.54

23 layer (flash attention) transformer (97198592 parameters), swiglu activation, rotary position embeddings, batch size 20, Adam learning rate 3e-4, dropout 0.0, weight decay 0.1, cosine learning rate schedule with warmup
epoch 1 loss: 2.23
epoch 2 loss: N/A, killed in favor of other experiments
epoch 3 loss: N/A, killed in favor of other experiments

465 ms/batch

6 layer (flash attention) transformer (2102272 parameters), swiglu activation, learned position embeddings, batch size 20, Adam learning rate 3e-4, dropout 0.0, weight decay 0.1, cosine learning rate schedule with warmup
epoch 1 loss: 2.58
epoch 2 loss: 2.55
epoch 3 loss: 2.53
